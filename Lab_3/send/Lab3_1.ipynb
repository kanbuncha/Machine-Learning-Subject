{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "colab_type": "code",
    "id": "H4-hyRIEk1ph",
    "outputId": "3bba3f4a-e229-47cb-c8b3-2559933fbb2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import talos as ta\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "### โหลดข้อมูล Timeseries Dataset file \n",
    "\n",
    "# num_name = ['46343', '759667', '781756', '844359', '1066528']\n",
    "# num_name = ['46343', '844359']\n",
    "# fea_name = [ 'acceleration', 'heartrate', 'steps', 'labeled_sleep']\n",
    "\n",
    "num_name = ['46343', '781756', '1066528', '844359']\n",
    "fea_name = [ 'acceleration', 'heartrate', 'labeled_sleep']\n",
    "\n",
    "df_list = list()\n",
    "merge_fea_col = pd.DataFrame()\n",
    "\n",
    "for n in num_name :\n",
    "  fea_col = list()\n",
    "  for idx_fea,f in enumerate(fea_name, 0): \n",
    "    fea_df = pd.read_csv(root_dir + '/Sleep_MiNiSet/' + n + '_' + f + '.txt', sep=\" \", header=None)\n",
    "    if len(fea_df.columns) < 2 :\n",
    "      fea_df = pd.read_csv(root_dir + '/Sleep_MiNiSet/' + n + '_' + f + '.txt', sep=\",\", header=None)\n",
    "    fea_df['uts_' + f] = pd.to_datetime(fea_df[0], unit='s', origin=pd.Timestamp('2019-10-04'))\n",
    "    if len(fea_df.columns) <= 3 :\n",
    "      fea_df.columns = ['time_sec_'+f , f, 'uts_' + f]\n",
    "    else :\n",
    "      fea_df.columns = ['time_sec_'+f , f+'_x', f+'_y', f+'_z', 'uts_' + f]  \n",
    "    fea_col.append(fea_df)\n",
    "\n",
    "  limit_sec = fea_col[len(fea_name) - 1].iloc[-1:, 0].values[0]\n",
    "  for num_df in range(len(fea_name)) : \n",
    "    fea_col[num_df].set_index('uts_' + fea_name[num_df], inplace=True)\n",
    "    fea_col[num_df] = fea_col[num_df][(fea_col[num_df]['time_sec_' + fea_name[num_df]] >= 0 ) & ( fea_col[num_df]['time_sec_' + fea_name[num_df]] <= limit_sec + 30)]\n",
    "    fea_col[num_df] = fea_col[num_df].resample('30s').mean()\n",
    "    fea_col[num_df].drop(fea_col[num_df].columns[0], axis=1, inplace=True)\n",
    "\n",
    "  df_list.append(pd.concat([fea_col[0], fea_col[1], fea_col[2]], axis=1))\n",
    "\n",
    "df_all = pd.concat([df_list[0], df_list[1], df_list[2], df_list[3]], sort=False)\n",
    "df_all.columns = ['acceleration_x', 'acceleration_y', 'acceleration_z', 'heartrate', 'labeled_sleep']\n",
    "df_all.describe()\n",
    "\n",
    "###  Preprocess data \n",
    "\n",
    "# reset index\n",
    "df_all.reset_index(inplace=True)\n",
    "df_all.columns = ['uts', 'acceleration_x', 'acceleration_y', 'acceleration_z', 'heartrate', 'labeled_sleep']\n",
    "\n",
    "before_clean_df = df_all\n",
    "\n",
    "# drop duplicate\n",
    "df_all.drop_duplicates(inplace=True)\n",
    "\n",
    "# drop row that label = -1\n",
    "df_all = df_all[df_all['labeled_sleep'] >= 0]\n",
    "\n",
    "# fill null value with med\n",
    "df_all.fillna(df_all.median()['acceleration_x': 'heartrate'], inplace=True)\n",
    "\n",
    "# แทรกข้อมูลในช่วงเวลาที่ขาดหายไป\n",
    "df_all.interpolate(method='slinear', inplace=True)\n",
    "\n",
    "# จัดการลดสัญญาณรบกวนในข้อมูลด้วยการทำ Moving Average \n",
    "df_all.rolling(2).mean()\n",
    "\n",
    "df_all.set_index('uts', inplace=True)\n",
    "df_all.describe()\n",
    "\n",
    "# visual เปรียบเทียบข้อมูลก่อน Cleaning\n",
    "\n",
    "before_clean_df.set_index('uts', inplace=True)\n",
    "before_clean_df[before_clean_df.columns].plot(subplots=True, layout=(2, 3), figsize=(15,10))\n",
    "\n",
    "g = sns.PairGrid(before_clean_df, hue='labeled_sleep')\n",
    "g = g.map_offdiag(plt.scatter)\n",
    "g = g.add_legend()\n",
    "g.map_upper(plt.scatter)\n",
    "g.map_lower(sns.kdeplot)\n",
    "g.map_diag(sns.distplot)\n",
    "\n",
    "# visual เปรียบเทียบข้อมูลหลัง Cleaning\n",
    "\n",
    "df_all[df_all.columns].plot(subplots=True, layout=(2, 3), figsize=(15,10))\n",
    "\n",
    "g = sns.PairGrid(df_all, hue='labeled_sleep')\n",
    "g = g.map_offdiag(plt.scatter)\n",
    "g = g.add_legend()\n",
    "g.map_upper(plt.scatter)\n",
    "g.map_lower(sns.kdeplot)\n",
    "g.map_diag(sns.distplot)\n",
    "\n",
    "# ทำการ Normalize ข้อมูลด้วยเทคนิค Max-Min Norm\n",
    "for column in df_all.loc[:, 'acceleration_x':'heartrate'].columns :\n",
    "    df_all[column] = df_all.apply(lambda row: (row[column] - df_all[column].min()) / (df_all[column].max() - df_all[column].min()), axis=1)\n",
    "df_all\n",
    "\n",
    "# สร้างฟังก์ชันสำหรับสร้าง TimeSeries\n",
    "def process_create_WindowTimeSeries(df, activity_start, activity_len, time_window, n_feature, step_stride):\n",
    "    df_series = df\n",
    "    segments = []\n",
    "    # วนลูปโดยให้ i มีค่าตั้งแต่ row ที่ 0 ถึง ( จำนวน row - time_window) โดยนับทีละ step_stride\n",
    "    for i in range(0, len(df_series) - time_window, step_stride):\n",
    "        \n",
    "        # เก็บค่าของ row ที่ i ถึง i + time_window\n",
    "        df_series_feature = df_series.iloc[i: i + time_window]   \n",
    "        segments.append(np.array(df_series_feature))\n",
    "        \n",
    "    # ทำการ reshape ให้มีขนาด  ( #ชุด time_series, #time_step, #features ) \n",
    "    reshaped_segments = np.asarray(segments).reshape(-1, time_window, n_feature)\n",
    "\n",
    "    return reshaped_segments\n",
    "\n",
    "time_step = 3\n",
    "time_stride = 1\n",
    "col_name = ['acceleration_x', 'acceleration_y', 'acceleration_z', 'heartrate', 'labeled_sleep']\n",
    "\n",
    "time_series = process_create_WindowTimeSeries(df_all[col_name], 0, len(col_name), time_step, len(col_name), time_stride)\n",
    "time_series_2d = time_series.reshape(time_series.shape[0]*time_series.shape[1],time_series.shape[2])\n",
    "\n",
    "print(time_series.shape)\n",
    "print('=================================')\n",
    "print(time_series_2d.shape)\n",
    "\n",
    "### Prepare Label Ground Truth (y) \n",
    "\n",
    "# Init X\n",
    "col_name = ['acceleration_x', 'acceleration_y', 'acceleration_z', 'heartrate']\n",
    "X = process_create_WindowTimeSeries(df_all[col_name], 0, len(col_name), time_step, len(col_name), time_stride)\n",
    "X_2d = X.reshape(X.shape[0]*X.shape[1], X.shape[2])  # convert to 2D\n",
    "\n",
    "print(X.shape)\n",
    "print(X_2d.shape)\n",
    "\n",
    "# Majority vote prepare Label Ground Truth (y)\n",
    "\n",
    "def majority_3d(time_series=time_series):\n",
    "  mj_list = list()\n",
    "\n",
    "  for t in range(time_series.shape[0]) :\n",
    "    mj_vote = list()\n",
    "    for r in range(time_series.shape[1]):\n",
    "      mj_vote.append( time_series[t][r][time_series.shape[2] - 1] )\n",
    "    mj_counter = Counter(mj_vote)\n",
    "    mj_list.append(mj_counter.most_common(1)[0][0])\n",
    "  return mj_list\n",
    "\n",
    "def majority_2d(time_series=time_series_2d):\n",
    "  mj_list = list()\n",
    "  for r in range(time_series.shape[0]) :\n",
    "    mj_list.append(time_series[r][-1])\n",
    "  return mj_list\n",
    "\n",
    "\n",
    "# Init y 1D\n",
    "y = np.array(majority_3d())\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Onehot\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "y = enc.transform(y).toarray()\n",
    "\n",
    "print(y.shape)\n",
    "print('=======================================================')\n",
    "\n",
    "# Init y 2D\n",
    "y_2d = np.array(majority_2d())\n",
    "y_2d = y_2d.reshape(-1, 1)\n",
    "\n",
    "# Onehot\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y_2d)\n",
    "y_2d = enc.transform(y_2d).toarray()\n",
    "\n",
    "print(y_2d.shape)\n",
    "\n",
    "### Prepare training, validation, and test data\n",
    "\n",
    "# # train test split\n",
    "\n",
    "# X_t, X_test, y_t, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_t, y_t, test_size=0.2, random_state=42, stratify=y_t)\n",
    "\n",
    "# X_2d_t, X_2d_test, y_2d_t, y_2d_test = train_test_split(X_2d, y_2d, test_size=0.2, random_state=42, stratify=y_2d)\n",
    "# X_2d_train, X_2d_valid, y_2d_train, y_2d_valid = train_test_split(X_2d_t, y_2d_t, test_size=0.2, random_state=42, stratify=y_2d_t)\n",
    "\n",
    "# X_train = np.expand_dims(X_train, axis=-1)\n",
    "# X_test = np.expand_dims(X_test, axis=-1)\n",
    "# X_valid = np.expand_dims(X_valid, axis=-1)\n",
    "\n",
    "# X_2d_train = np.expand_dims(X_2d_train, axis=-1)\n",
    "# X_2d_test = np.expand_dims(X_2d_test, axis=-1)\n",
    "# X_2d_valid = np.expand_dims(X_2d_valid, axis=-1)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(X_valid.shape)\n",
    "\n",
    "# print('=============================')\n",
    "# print(X_2d_train.shape)\n",
    "# print(X_2d_test.shape)\n",
    "# print(X_2d_valid.shape)\n",
    "\n",
    "# train test split\n",
    "\n",
    "X_t, X_test, y_t, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_t, y_t, test_size=0.2, random_state=42, stratify=y_t)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "X_valid = np.expand_dims(X_valid, axis=-1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_valid.shape)\n",
    "\n",
    "print('=============================')\n",
    "\n",
    "\n",
    "X_2d_train  = X_train.reshape(-1, 3, 4) \n",
    "X_2d_valid  = X_valid.reshape(-1, 3, 4) \n",
    "X_2d_test  = X_test.reshape(-1, 3, 4) \n",
    "\n",
    "print(X_2d_train.shape)\n",
    "print(X_2d_test.shape)\n",
    "print(X_2d_valid.shape)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
